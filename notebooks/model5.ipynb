{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import bz2\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])\n",
    "import gensim\n",
    "\n",
    "# from textstat.textstat import textstat\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from nlgeval import compute_metrics\n",
    "# import kenlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to read and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract complex and simple sentences and text category \n",
    "# from all data for one sentence in DBPedia dataset\n",
    "\n",
    "def parseSentenceData(data):\n",
    "    data = data.strip().split(\"\\n\\n\")\n",
    "    \n",
    "    complexsentdata = data[0].strip().split(\"\\n\")\n",
    "    complexid = int(complexsentdata[0].split(\"-\")[1].strip())\n",
    "    complexsent = complexsentdata[1].strip()\n",
    "\n",
    "    cat = '' \n",
    "    mr_dict = {}\n",
    "    # Collect all complex mrs\n",
    "    for item in data[1:]:\n",
    "        if re.match('COMPLEX-'+str(complexid)+':MR-[0-9]*\\n', item):\n",
    "            # print item\n",
    "            mrdata = item.strip().split(\"\\n\")\n",
    "            mrid = mrdata[0]\n",
    "            mr = mrdata[1]\n",
    "            if re.match('category=[a-zA-Z]* eid=Id[0-9]* size=[0-9]*', mr):\n",
    "                cat = mr.strip().split(' ')[0].split('=')[1]\n",
    "            mr_dict[mrid] = [mr, {}]\n",
    "    \n",
    "    simpsents = {}\n",
    "    for item in data[1:]:\n",
    "        if re.match('COMPLEX-'+str(complexid)+':MR-[0-9]*:SIMPLE-[0-9]*\\n', item):\n",
    "            \n",
    "            mrid = \":\".join(item.strip().split(\"\\n\")[0].split(\":\")[:2])\n",
    "\n",
    "            sents = (\" \".join(item.strip().split(\"\\n\")[1:])).strip()\n",
    "            \n",
    "            if sents not in simpsents:\n",
    "                simpsents[sents] = 1\n",
    "\n",
    "            if sents not in mr_dict[mrid][1]:\n",
    "                mr_dict[mrid][1][sents] = 1\n",
    "\n",
    "    return complexsent, simpsents, cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove some frequent garbage from sentences \n",
    "def preprocessSentence(text):\n",
    "    text = re.sub('-LRB-(.*?)-RRB-', '', text)\n",
    "    text = re.sub('-RRB-', '', text)\n",
    "    text = re.sub('^\\\"', '', text)\n",
    "    text = re.sub('\\\"$', '', text)\n",
    "    text = re.sub('\\\\n', '', text)\n",
    "    text = re.sub('\\\\r', '', text)\n",
    "    text = re.sub('\\\\t', '', text)\n",
    "    text = re.sub('\\`', '', text)\n",
    "    text = re.sub('\\'\\'', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read DBPedia dataset\n",
    "def readDBPediaData(compl_to_sim):\n",
    "    filename = '../dataset/complexsimple.txt.bz2'\n",
    "    with bz2.BZ2File(filename, \"r\") as content:         \n",
    "        sentdata = []\n",
    "        i = 0\n",
    "        close_to_geo_categories = ['Monument', 'City', 'Airport', 'Building']\n",
    "\n",
    "        for line in content:\n",
    "            if len(sentdata) == 0:\n",
    "                sentdata.append(line)\n",
    "            else:\n",
    "                if re.match('COMPLEX-[0-9]*\\n', line):\n",
    "                    complexsent, simpsents, cat = parseSentenceData(\"\".join(sentdata))\n",
    "                    simple_sen = simpsents.keys()[0]\n",
    "                    \n",
    "                    if (cat in close_to_geo_categories) and (len(simple_sen) > 0):\n",
    "                        compl_to_sim[preprocessSentence(complexsent)] = preprocessSentence(simple_sen)\n",
    "                    sentdata = [line]\n",
    "                else:\n",
    "                    sentdata.append(line)\n",
    "    return compl_to_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read Newsela dataset\n",
    "def readNewselaData(compl_to_sim):\n",
    "    with bz2.BZ2File('../dataset/clear_newsela.txt.bz2', \"r\") as content:         \n",
    "        for line in content:\n",
    "            texts = line.split('\\t')\n",
    "\n",
    "            if len(texts) != 3:\n",
    "                continue\n",
    "            if (len(texts[0]) < 10) or (len(texts[1]) < 10):\n",
    "                continue\n",
    "                \n",
    "            compl_to_sim[preprocessSentence(texts[0])] = preprocessSentence(texts[1])\n",
    "    return compl_to_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../dataset/tokenized_geougraphy.txt\", \"w\") as f:\n",
    "    for com, sim in tokenized_texts.iteritems():\n",
    "        f.write(com + '\\t' + sim + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data to train and test set\n",
    "def splitTrainTest(compl_to_sim):\n",
    "    train = {}\n",
    "    test = {}\n",
    "\n",
    "    for compl, sim in compl_to_sim.iteritems():\n",
    "        try:\n",
    "            sim_u = sim.encode('utf-8')\n",
    "            compl_u = compl.encode('utf-8')\n",
    "        except:\n",
    "            continue\n",
    "        r = random.random()\n",
    "        if r < 0.8:\n",
    "            train[compl] = sim\n",
    "        else:\n",
    "            test[compl] = sim\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for baseline logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some words does not make sense (or change sense) when used without dependent clause.\n",
    "# For example, \"part\" in \"part of the U.S.A.\" or \"length\" in \"length of 4000\".\n",
    "# This function builds a clause required to save such information.\n",
    "\n",
    "def getReasonableChunk(word, doc):\n",
    "    chunk_words = []\n",
    "    if word.pos_ == 'PROPN':\n",
    "        try:\n",
    "            chunks = list(x for x in doc.noun_chunks if x.root == word)[0]\n",
    "        except:\n",
    "            chunks = [word]\n",
    "        chunk_words.extend(list(x for x in chunks))\n",
    "        inner_conj = [x for x in word.children if x.dep_ == 'cc']\n",
    "        if len(inner_conj) > 0:\n",
    "            chunk_words.extend(inner_conj)\n",
    "            for conj_propn in list(x for x in word.children if (x.dep_ == 'conj') and (x.pos_ == 'PROPN')):\n",
    "                chunk_words.extend(getReasonableChunk(conj_propn, doc))\n",
    "\n",
    "    elif word.pos_ == 'NOUN':\n",
    "        nummods = list(x for x in word.children if x.dep_ == 'nummod')\n",
    "        chunk_words.append(word)\n",
    "        chunk_words.extend(nummods)\n",
    "        \n",
    "        dets = list(x for x in word.children if x.dep_ == 'det')\n",
    "        chunk_words.extend(dets)\n",
    "        \n",
    "        compunds = list(x for x in word.children if x.dep_ == 'compound')\n",
    "#         chunk_words.extend(compunds)\n",
    "        for c in compunds:\n",
    "            chunk_words.extend(getReasonableChunk(c, doc))\n",
    "    elif word.text.lower() in ['which', 'that']:\n",
    "        replacer = replaceWhich(word, doc)\n",
    "        if replacer is not None:\n",
    "            chunk_words.extend(getReasonableChunk(replacer, doc))\n",
    "        else:\n",
    "            chunk_words.append(word)\n",
    "    elif word.text.lower() == 'who':\n",
    "        replacer = replaceWhich(word, doc)\n",
    "        if replacer is not None:\n",
    "            chunk_words.extend(getReasonableChunk(replacer, doc))     \n",
    "        else:\n",
    "            chunk_words.append(word)\n",
    "    else:\n",
    "        chunk_words.append(word)\n",
    "\n",
    "        \n",
    "    prepositions = list(x for x in word.children if x.dep_ == 'prep')\n",
    "    pobjects = []\n",
    "    for prep in prepositions:\n",
    "        pobjects = list(x for x in prep.children if x.dep_ == 'pobj')\n",
    "        if len(pobjects) > 0:\n",
    "            chunk_words.append(prep)\n",
    "            for s in pobjects:\n",
    "                chunk_words.extend(getReasonableChunk(s, doc))\n",
    "                    \n",
    "    return chunk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert list of lists of words to text (form readable sentences)\n",
    "def wordsToSimpleText(sentence_words):\n",
    "    text = ''\n",
    "    for words in sentence_words:\n",
    "        sentence_text = ''\n",
    "        for word in words:\n",
    "            sentence_text += word.text + ' '\n",
    "            \n",
    "        if len(sentence_text) > 0:\n",
    "            sentence_text = sentence_text[0].upper() + sentence_text[1:]\n",
    "            sentence_text += '. '\n",
    "            \n",
    "            text += sentence_text\n",
    "            \n",
    "    text = text.replace(\" which \", \" this \")\n",
    "    text = text.replace(\"Which \", \"This \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main baseline function\n",
    "# build set of simple sentences based on each verb's subtree in a complex sentence\n",
    "# input: spacy.doc with complex sentences\n",
    "# output: text with one or more simple sentences\n",
    "\n",
    "def subjVerbObjToSentence(sentence, verb_simple_texts):\n",
    "    verbs = list(x for x in sentence if x.pos_ == 'VERB')\n",
    "    new_sentences = []\n",
    "    for v in verbs:\n",
    "        new_sent_words = [v]\n",
    "        has_sence = False\n",
    "        \n",
    "        ouxilaries = list(x for x in v.children if (x.dep_ == 'aux') or (x.dep_ == 'auxpass'))\n",
    "        new_sent_words.extend(ouxilaries)\n",
    "        \n",
    "        print 'ouxilaries', new_sent_words\n",
    "        subjects = list(x for x in v.children if (x.dep_ == 'nsubj') or (x.dep_ == 'nsubjpass'))\n",
    "        if len(subjects) > 0:\n",
    "            has_sence = True\n",
    "            for s in subjects:\n",
    "                new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "        print 'subjects', new_sent_words\n",
    "        attributes = list(x for x in v.children if (x.dep_ == 'attr'))\n",
    "        if len(attributes) > 0:\n",
    "            has_sence = True\n",
    "            for s in attributes:\n",
    "                new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "        print 'attributes', new_sent_words\n",
    "        negs = list(x for x in v.children if (x.dep_ == 'neg'))\n",
    "        if len(negs) > 0:\n",
    "            for s in negs:\n",
    "                new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "        print 'neg', new_sent_words        \n",
    "        dobjects = list(x for x in v.children if x.dep_ in ['dobj', 'oprd', 'acomp'])\n",
    "        if len(dobjects) > 0:\n",
    "            has_sence = True\n",
    "            for s in dobjects:\n",
    "                new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "        print 'dobjects', new_sent_words    \n",
    "        prepositions = list(x for x in v.children if x.dep_ == 'prep')\n",
    "        pobjects = []\n",
    "        for prep in prepositions:\n",
    "            pobjects = list(x for x in prep.children if x.dep_ == 'pobj')\n",
    "            if len(pobjects) > 0:\n",
    "                has_sence = True\n",
    "                new_sent_words.append(prep)\n",
    "                for s in pobjects:\n",
    "                    new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "        print 'pobjects', new_sent_words           \n",
    "        agents = list(x for x in v.children if x.dep_ == 'agent')\n",
    "        pobjects = []\n",
    "        for prep in agents:\n",
    "            pobjects = list(x for x in prep.children if x.dep_ == 'pobj')\n",
    "            if len(pobjects) > 0:\n",
    "                has_sence = True\n",
    "                new_sent_words.append(prep)\n",
    "                for s in pobjects:\n",
    "                    new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "        print 'pobjects', new_sent_words\n",
    "        if (v.dep_ == 'conj') and (len([x for x in v.children if x.dep_ in ['nsubj', 'nsubjpass']]) == 0):\n",
    "            subjects = list(x for x in v.head.children if (x.dep_ in ['nsubj', 'nsubjpass', 'aux']))\n",
    "            if len(subjects) > 0:\n",
    "                has_sence = True\n",
    "                for s in subjects:\n",
    "                    new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "        print 'subjects', new_sent_words\n",
    "#         print v.dep_\n",
    "        if v.dep_ in ['xcomp', 'advcl']:\n",
    "            h = v\n",
    "            i = 0\n",
    "            while (True):\n",
    "#                 print h.text\n",
    "                i += 1\n",
    "                if i > 5 : \n",
    "                    break\n",
    "                h = h.head\n",
    "                new_sent_words.append(h)\n",
    "                \n",
    "                if h.pos_ == 'VERB':\n",
    "                    has_sence = True\n",
    "                    break\n",
    "        print 'parent verb', new_sent_words\n",
    "        \n",
    "#         new_sent_words = list(set(new_sent_words))\n",
    "        new_sent_words = sorted(new_sent_words, key=lambda x: x.i)\n",
    "        \n",
    "        if has_sence == True:\n",
    "            new_sentences.append(new_sent_words)\n",
    "        if has_sence == True:\n",
    "            simple_text = wordsToSimpleText([new_sent_words])\n",
    "            if v.dep_ == 'acl':\n",
    "                simple_text = \"It is \" + simple_text\n",
    "            verb_simple_texts.append(simple_text)\n",
    "    return verb_simple_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjNounToSentence(sentence, noun_simple_texts):\n",
    "    nouns = list(x for x in sentence if x.pos_ == 'NOUN')\n",
    "    for n in nouns:\n",
    "#         print n.text\n",
    "        new_sent_words = [n]\n",
    "        has_sence = False\n",
    "        \n",
    "        optional_mods = list(x for x in n.children if (x.dep_ == 'amod') and (x.pos_ in ['ADJ', 'VERB']))\n",
    "\n",
    "        verb = \"is\"\n",
    "        if n.tag_ == 'NNS':\n",
    "            verb = \"are\"\n",
    "                \n",
    "        add_mods = []\n",
    "        for mod in optional_mods:\n",
    "            add_mods.extend(list(x for x in mod.subtree))\n",
    "\n",
    "        if len(add_mods) > 0:\n",
    "            new_sent_words = sorted(add_mods, key=lambda x: x.i)\n",
    "            noun_chunk = [n]\n",
    "            noun_chunk = sorted(noun_chunk, key=lambda x: x.i)\n",
    "\n",
    "            new_sent_texts = [x.text for x in noun_chunk]\n",
    "            new_sent_texts.append(verb)\n",
    "            new_sent_texts.extend([x.text for x in new_sent_words])\n",
    "\n",
    "            sent_text = ' '.join(new_sent_texts)\n",
    "            sent_text = sent_text[0].upper() + sent_text[1:]\n",
    "            sent_text += '. '\n",
    "\n",
    "            noun_simple_texts[n.text] = sent_text\n",
    "\n",
    "\n",
    "        appos = list(x for x in n.children if (x.dep_ == 'appos'))\n",
    "        for ap in appos:\n",
    "            new_sent_words = sorted([x for x in ap.subtree], key=lambda x: x.i)\n",
    "            sent_text = n.text + ' ' + verb + ' ' + ' '.join([x.text for x in new_sent_words]) + '. '\n",
    "            sent_text = sent_text[0].upper() + sent_text[1:]\n",
    "            noun_simple_texts[n.text] = sent_text\n",
    "            \n",
    "        in_prep = list(x for x in n.children if (x.dep_ == 'prep') and (x.text in ['in', 'at', 'on']))\n",
    "#         print in_prep\n",
    "        for ap in in_prep:\n",
    "            pobjs = list(x for x in ap.children if x.dep_ == 'pobj')\n",
    "            for p in pobjs:\n",
    "                new_sent_words = sorted([x for x in p.subtree], key=lambda x: x.i)\n",
    "                sent_text = n.text + ' ' + verb + ' ' + ap.text + ' ' + ' '.join([x.text for x in new_sent_words]) + '. '\n",
    "                sent_text = sent_text[0].upper() + sent_text[1:]\n",
    "                noun_simple_texts[n.text] = sent_text\n",
    "            \n",
    "    return noun_simple_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propnConjToSentence(sentence, noun_simple_texts):\n",
    "    nouns = list(x for x in sentence if x.pos_ == 'PROPN')\n",
    "    parts = [' is in ', ' is located in ']\n",
    "    for n in nouns:\n",
    "        appos = list(x for x in n.children if (x.dep_ == 'appos'))\n",
    "        for ap in appos:\n",
    "            new_sent_words = sorted([x for x in ap.subtree], key=lambda x: x.i)\n",
    "            sent_text = n.text + ' is ' + ' '.join([x.text for x in new_sent_words]) + '. '\n",
    "            sent_text = sent_text[0].upper() + sent_text[1:]\n",
    "            noun_simple_texts[n.text] = sent_text\n",
    "            \n",
    "        if len([x for x in n.children if x.dep_ == 'cc']) > 0:\n",
    "            continue\n",
    "        for aggr in list(x for x in n.children if (x.dep_ in ['conj', 'appos']) and (x.pos_ == 'PROPN')):\n",
    "        \n",
    "            sent_text = n.text + random.choice(parts) + aggr.text + '. '\n",
    "\n",
    "            noun_simple_texts[n.text] = sent_text\n",
    "        \n",
    "\n",
    "        \n",
    "    return noun_simple_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceWhich(token, doc):\n",
    "    steps = 0\n",
    "    while token.dep_ != 'root':\n",
    "        steps += 1\n",
    "        if steps > 50: \n",
    "            break\n",
    "        token = token.head\n",
    "        if token.pos_ in ['NOUN', 'PROPN']:\n",
    "            return token\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replaceWho(token, doc):\n",
    "    steps = 0\n",
    "    while token.dep_ != 'root':\n",
    "        steps += 1\n",
    "        if steps > 50: \n",
    "            break\n",
    "        token = token.head\n",
    "        if token.pos_ in ['PROPN']:\n",
    "            return token\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compl, sim = random.choice(list(tokenized_texts.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouxilaries [is]\n",
      "subjects [is, WazaNationalPark]\n",
      "attributes [is, WazaNationalPark]\n",
      "neg [is, WazaNationalPark]\n",
      "dobjects [is, WazaNationalPark]\n",
      "pobjects [is, WazaNationalPark, in, north, the]\n",
      "pobjects [is, WazaNationalPark, in, north, the]\n",
      "subjects [is, WazaNationalPark, in, north, the]\n",
      "parent verb [is, WazaNationalPark, in, north, the]\n",
      "----\n",
      "[u'WazaNationalPark is in the north . ']\n",
      "----\n",
      "{}\n",
      "\n",
      "\n",
      "complex sentence: \n",
      "WazaNationalPark is in the north .\n",
      "simple_text_given: \n",
      "WazaNationalPark is a good place to see wildlife in Cameroon .\n",
      "simple_text_generated: \n",
      "WazaNationalPark is in the north . \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "doc = nlp(compl.decode('utf-8'), disable=['ner', 'textcat'])\n",
    "verb_s_texts = []\n",
    "verb_s_texts = subjVerbObjToSentence(doc, verb_s_texts)\n",
    "noun_s_texts = {}\n",
    "noun_s_texts = adjNounToSentence(doc, noun_s_texts)\n",
    "noun_s_texts = propnConjToSentence(doc, noun_s_texts)\n",
    "print '----'\n",
    "print verb_s_texts\n",
    "print '----'\n",
    "print noun_s_texts\n",
    "\n",
    "simple_text = ''\n",
    "last_found = 0\n",
    "for sent in verb_s_texts:\n",
    "    simple_text += sent\n",
    "    st = sent.lower()\n",
    "    for n in noun_s_texts.keys():\n",
    "        if (st.find(n.lower()) >= 0):\n",
    "            simple_text += noun_s_texts[n]\n",
    "            noun_s_texts[n] = ''\n",
    "            break\n",
    "for n in noun_s_texts.keys():\n",
    "    if len(noun_s_texts[n]) > 0:\n",
    "        simple_text += noun_s_texts[n]\n",
    "print '\\n\\ncomplex sentence: '\n",
    "print doc.text\n",
    "print 'simple_text_given: '\n",
    "print sim\n",
    "print 'simple_text_generated: '\n",
    "print simple_text\n",
    "#     print 'rejected sentences: '\n",
    "#     print rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8705"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## main1 ##\n",
    "compl_to_sim = {}\n",
    "compl_to_sim = readNewselaData(compl_to_sim)\n",
    "compl_to_sim = readDBPediaData(compl_to_sim)\n",
    "len(compl_to_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "compl_to_sim = {}\n",
    "with open(\"../dataset/texts_geougraphy.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        texts = line.split('\\t')\n",
    "\n",
    "        if len(texts) != 2:\n",
    "            continue\n",
    "        try:\n",
    "            sim_u = texts[0].encode('utf-8')\n",
    "            compl_u = texts[1].encode('utf-8')\n",
    "        except:\n",
    "            continue\n",
    "        compl_to_sim[texts[0]] = texts[1]\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8395"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(compl_to_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Atlantic City International Airport can be found in Egg Harbor Township , New Jersey in the U.S.A.',\n",
       " 'Atlantic City International Airport is located at Egg Harbor Township , New Jersey . Egg Harbor Township is a part of New Jersey . Egg Harbor Township is located in New Jersey , in the United States .')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(compl_to_sim.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_com_words = {}\n",
    "all_sim_words = {}\n",
    "tokenized_texts = {}\n",
    "for com, sim in compl_to_sim.iteritems():\n",
    "    text = nlp(com.decode('utf-8'))\n",
    "    text_tokens = []\n",
    "    propn_text = ''\n",
    "    for token in text:\n",
    "        if (token.pos_ == 'PROPN'):\n",
    "            propn_text += token.text\n",
    "        elif len(propn_text) > 0:\n",
    "            text_tokens.append(propn_text)\n",
    "            propn_text = ''\n",
    "        if token.pos_ not in ['SPACE', 'SYM', 'X', 'PROPN']:\n",
    "            text_tokens.append(token.text)\n",
    "                \n",
    "    for tok in text_tokens:\n",
    "        if not all_com_words.has_key(tok.lower()):\n",
    "            all_com_words[tok.lower()] = 0\n",
    "        all_com_words[tok.lower()] += 1\n",
    "    \n",
    "    com_text = ' '.join(text_tokens)\n",
    "\n",
    "    \n",
    "    text = nlp(sim.decode('utf-8'))\n",
    "    text_tokens = []\n",
    "    propn_text = ''\n",
    "    for token in text:\n",
    "        if (token.pos_ == 'PROPN'):\n",
    "            propn_text += token.text\n",
    "        elif len(propn_text) > 0:\n",
    "            text_tokens.append(propn_text)\n",
    "            propn_text = ''\n",
    "        if token.pos_ not in ['SPACE', 'SYM', 'X', 'PROPN']:\n",
    "            text_tokens.append(token.text)\n",
    "                \n",
    "    for tok in text_tokens:\n",
    "        if not all_sim_words.has_key(tok.lower()):\n",
    "            all_sim_words[tok.lower()] = 0\n",
    "        all_sim_words[tok.lower()] += 1\n",
    "    \n",
    "    sim_text = ' '.join(text_tokens)\n",
    "    \n",
    "    tokenized_texts[com_text] = sim_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8325"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'The horns were carved into spoons and ladles , the hooves cooked to make glue .',\n",
       " u'They used buffalo horns to make spoons .')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(list(tokenized_texts.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = splitTrainTest(compl_to_sim)\n",
    "print len(train), len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get some examples of baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyText(object):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for com, sim in self.dataset.iteritems():\n",
    "            text = nlp((com + ' ' + sim).decode('utf-8'))\n",
    "            for sentence in text.sents:\n",
    "                propn_text = ''\n",
    "                text_tokens = []\n",
    "                for token in sentence:\n",
    "                    if (token.pos_ == 'PROPN'):\n",
    "                        propn_text += token.text\n",
    "                    elif len(propn_text) > 0:\n",
    "                        text_tokens.append(propn_text)\n",
    "                        propn_text = ''\n",
    "                    if token.pos_ not in ['SPACE', 'PUNCT', 'SYM', 'X', 'NUM', 'PROPN']:\n",
    "                        text_tokens.append(token.text.lower())\n",
    "                yield text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = MyText(compl_to_sim)\n",
    "model = gensim.models.Word2Vec(data, size=100, min_count=2, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'leader', 0.6677063703536987),\n",
       " (u'AsianAmericans', 0.6671854257583618),\n",
       " (u'group', 0.6590251922607422),\n",
       " (u'groups', 0.6352567076683044),\n",
       " (u'EthiopianBirr', 0.6279417872428894),\n",
       " (u'AfricanAmericans', 0.6198752522468567),\n",
       " (u'ethnic', 0.6174106597900391),\n",
       " (u'language', 0.6029496192932129),\n",
       " (u'capital', 0.5959588289260864),\n",
       " (u'state', 0.5826288461685181),\n",
       " (u'SouthAfrica', 0.5808447003364563),\n",
       " (u'leaders', 0.5779481530189514),\n",
       " (u'RioSolimoes', 0.5771121978759766),\n",
       " (u'Oregon', 0.5763739347457886),\n",
       " (u'region', 0.5747165679931641),\n",
       " (u'city', 0.5572177767753601),\n",
       " (u'English', 0.5509083867073059),\n",
       " (u'President', 0.5377963185310364),\n",
       " (u'Japan', 0.5365291833877563),\n",
       " (u'U.S.AsianAmericans', 0.5361120700836182)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"country\"],   topn=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
