{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import bz2\n",
    "import spacy\n",
    "nlp = spacy.load('en', disable=['ner', 'textcat', 'depend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to read and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract complex and simple sentences and text category \n",
    "# from all data for one sentence in DBPedia dataset\n",
    "\n",
    "def parseSentenceData(data):\n",
    "    data = data.strip().split(\"\\n\\n\")\n",
    "    \n",
    "    complexsentdata = data[0].strip().split(\"\\n\")\n",
    "    complexid = int(complexsentdata[0].split(\"-\")[1].strip())\n",
    "    complexsent = complexsentdata[1].strip()\n",
    "\n",
    "    cat = '' \n",
    "    mr_dict = {}\n",
    "    # Collect all complex mrs\n",
    "    for item in data[1:]:\n",
    "        if re.match('COMPLEX-'+str(complexid)+':MR-[0-9]*\\n', item):\n",
    "            # print item\n",
    "            mrdata = item.strip().split(\"\\n\")\n",
    "            mrid = mrdata[0]\n",
    "            mr = mrdata[1]\n",
    "            if re.match('category=[a-zA-Z]* eid=Id[0-9]* size=[0-9]*', mr):\n",
    "                cat = mr.strip().split(' ')[0].split('=')[1]\n",
    "            mr_dict[mrid] = [mr, {}]\n",
    "    \n",
    "    simpsents = {}\n",
    "    for item in data[1:]:\n",
    "        if re.match('COMPLEX-'+str(complexid)+':MR-[0-9]*:SIMPLE-[0-9]*\\n', item):\n",
    "            \n",
    "            mrid = \":\".join(item.strip().split(\"\\n\")[0].split(\":\")[:2])\n",
    "\n",
    "            sents = (\" \".join(item.strip().split(\"\\n\")[1:])).strip()\n",
    "            \n",
    "            if sents not in simpsents:\n",
    "                simpsents[sents] = 1\n",
    "\n",
    "            if sents not in mr_dict[mrid][1]:\n",
    "                mr_dict[mrid][1][sents] = 1\n",
    "\n",
    "    return complexsent, simpsents, cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some frequent garbage from sentences \n",
    "def preprocessSentence(text):\n",
    "    text = re.sub('-LRB-(.*?)-RRB-', '', text)\n",
    "    text = re.sub('^\\\"', '', text)\n",
    "    text = re.sub('\\\"$', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DBPedia dataset\n",
    "def readDBPediaData(compl_to_sim):\n",
    "    filename = '../dataset/complexsimple.txt.bz2'\n",
    "    with bz2.BZ2File(filename, \"r\") as content:         \n",
    "        sentdata = []\n",
    "        i = 0\n",
    "        close_to_geo_categories = ['Monument', 'City', 'Airport', 'Building']\n",
    "\n",
    "        for line in content:\n",
    "            if len(sentdata) == 0:\n",
    "                sentdata.append(line)\n",
    "            else:\n",
    "                if re.match('COMPLEX-[0-9]*\\n', line):\n",
    "                    complexsent, simpsents, cat = parseSentenceData(\"\".join(sentdata))\n",
    "                    simple_sen = simpsents.keys()[0]\n",
    "                    \n",
    "                    if (len(simple_sen) > 0):\n",
    "                        compl_to_sim[preprocessSentence(complexsent)] = preprocessSentence(simple_sen)\n",
    "                    sentdata = [line]\n",
    "                else:\n",
    "                    sentdata.append(line)\n",
    "    return compl_to_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Newsela dataset\n",
    "def readNewselaData(compl_to_sim):\n",
    "    with bz2.BZ2File('../dataset/clear_newsela.txt.bz2', \"r\") as content:         \n",
    "        for line in content:\n",
    "            texts = line.split('\\t')\n",
    "\n",
    "            if len(texts) != 3:\n",
    "#                 print 'wrong line: ', len(texts), line\n",
    "                continue\n",
    "            if (len(texts[0]) < 10) or (len(texts[1]) < 10):\n",
    "#                 print 'empty line: ', line\n",
    "                continue\n",
    "                \n",
    "            compl_to_sim[preprocessSentence(texts[0])] = preprocessSentence(texts[1])\n",
    "    return compl_to_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDependencyRepresentation(compl_to_sim_data):\n",
    "    train = {}\n",
    "    test = {}\n",
    "    i = 1\n",
    "\n",
    "    skip_pos = ['PUNCT', 'SPACE', 'X']\n",
    "    for compl, sim in compl_to_sim_data.iteritems():\n",
    "        try:\n",
    "            doc_c = nlp(compl.decode('utf-8'))\n",
    "            doc_s = nlp(sim.decode('utf-8'))\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        len_c = len(list(x for x in doc_c))\n",
    "        len_s = len(list(x for x in doc_s))\n",
    "        if (len_c > 50) or (len_s > 70) or (len_s <= len_c):\n",
    "            continue \n",
    "            \n",
    "        c_seq = [x.pos_ + 'O' + x.dep_ for x in doc_c if x.pos_ not in skip_pos]\n",
    "        s_seq = [x.pos_ + 'O' + x.dep_ for x in doc_s if x.pos_ not in skip_pos]\n",
    "        \n",
    "        if (len(c_seq) <= 1) or (len(s_seq) <= 1):\n",
    "            continue\n",
    "            \n",
    "        r = random.random()\n",
    "        if r < 0.8:\n",
    "            train[i] = {}\n",
    "            train[i][\"compl\"] = c_seq\n",
    "            train[i][\"sim\"] = s_seq\n",
    "            train[i][\"com_text\"] = compl\n",
    "            train[i][\"sim_text\"] = sim\n",
    "        else:\n",
    "            test[i] = {}\n",
    "            test[i][\"compl\"] = c_seq\n",
    "            test[i][\"sim\"] = s_seq\n",
    "            test[i][\"com_text\"] = compl\n",
    "            test[i][\"sim_text\"] = sim\n",
    "            \n",
    "        i += 1\n",
    "    return train, test\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## main1 ##\n",
    "compl_to_sim = {}\n",
    "compl_to_sim = readNewselaData(compl_to_sim)\n",
    "compl_to_sim = readDBPediaData(compl_to_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17867 4441\n"
     ]
    }
   ],
   "source": [
    "train, test = getDependencyRepresentation(compl_to_sim)\n",
    "print len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sim_text': 'The leader is in charge of the government .', 'sim': [u'DETOOOdet', u'NOUNOOOnsubj', u'VERBOOOROOT', u'ADPOOOprep', u'NOUNOOOpobj', u'ADPOOOprep', u'DETOOOdet', u'NOUNOOOpobj'], 'compl': [u'DETOOOdet', u'ADJOOOamod', u'NOUNOOOnsubj', u'VERBOOOROOT', u'NOUNOOOattr', u'ADPOOOprep', u'NOUNOOOpobj'], 'com_text': 'The prime minister is head of government . '}\n"
     ]
    }
   ],
   "source": [
    "print train[2]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 279\n",
      "English Max Length: 46\n",
      "English Vocabulary Size: 244\n",
      "English Max Length: 94\n"
     ]
    }
   ],
   "source": [
    "compl_tokenizer = create_tokenizer([' '.join(x[\"compl\"]) for i, x in train.iteritems()])\n",
    "compl_vocab_size = len(compl_tokenizer.word_index) + 1\n",
    "compl_length = max(len(x[\"compl\"]) for i, x in train.iteritems())\n",
    "print('English Vocabulary Size: %d' % compl_vocab_size)\n",
    "print('English Max Length: %d' % (compl_length))\n",
    "\n",
    "sim_tokenizer = create_tokenizer([' '.join(x[\"sim\"]) for i, x in train.iteritems()])\n",
    "sim_vocab_size = len(sim_tokenizer.word_index) + 1\n",
    "sim_length = max(len(x[\"sim\"]) for i, x in train.iteritems())\n",
    "print('English Vocabulary Size: %d' % sim_vocab_size)\n",
    "print('English Max Length: %d' % (sim_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'cconjooopreconj': 135, u'verbooonummod': 185, u'advoooexpl': 60, u'verboooaux': 30, u'detoooattr': 118, u'adpoooaux': 170, u'verbooooprd': 173, u'nounooonsubj': 8, u'adjooonsubjpass': 57, u'verboooroot': 3, u'adjooopobj': 58, u'adpoooagent': 21, u'advooooprd': 234, u'propnooodative': 155, u'adjoooappos': 123, u'pronooonsubj': 28, u'advooopcomp': 124, u'adjooomark': 214, u'intjooocompound': 150, u'symooopunct': 41, u'verbooocsubj': 74, u'nounoooattr': 10, u'nounoooquantmod': 230, u'propnoooacomp': 88, u'numooonsubj': 52, u'partoooamod': 226, u'detoooquantmod': 138, u'detooodobj': 119, u'numooodobj': 98, u'detooonummod': 191, u'adjooopcomp': 222, u'verboooauxpass': 9, u'partooocase': 20, u'nounooointj': 224, u'detooodet': 4, u'symooocc': 206, u'adjoooccomp': 114, u'detooodep': 139, u'adjooooprd': 100, u'adpoooprep': 2, u'nounoooxcomp': 187, u'detoooconj': 216, u'propnoooprep': 75, u'advoooneg': 66, u'nounooocsubj': 162, u'nounooonpadvmod': 44, u'adjoooroot': 94, u'nounooonummod': 195, u'adpooodative': 92, u'verbooocompound': 72, u'numooonmod': 220, u'adjoooacomp': 38, u'propnooonpadvmod': 42, u'partoooadvmod': 141, u'advoooattr': 212, u'nounooopcomp': 196, u'adjoooposs': 37, u'verbooocsubjpass': 223, u'verboooprep': 93, u'detooopobj': 117, u'pronooonpadvmod': 189, u'propnooomark': 126, u'propnooonummod': 159, u'numooocompound': 67, u'intjoooprep': 225, u'propnooopcomp': 190, u'adpoooappos': 163, u'advooocompound': 96, u'propnooodet': 77, u'verboooappos': 183, u'propnooodep': 111, u'numoooprep': 36, u'numooonsubjpass': 95, u'verbooopobj': 107, u'advooonsubj': 80, u'adpoooquantmod': 84, u'partoooprt': 62, u'adjooointj': 203, u'partoooroot': 199, u'propnoooadvmod': 99, u'verbooorelcl': 26, u'adjooocsubj': 217, u'adjooocompound': 31, u'propnooooprd': 54, u'adjoooadvmod': 78, u'adpooomark': 56, u'propnooonsubjpass': 12, u'advoooroot': 160, u'numooooprd': 153, u'adjooonsubj': 43, u'adpooointj': 232, u'nounoooacomp': 113, u'numooointj': 167, u'propnooopunct': 73, u'numoooamod': 218, u'nounooodative': 200, u'nounooodep': 91, u'adpoooadvmod': 104, u'verboooxcomp': 51, u'verboooposs': 231, u'nounoooadvmod': 90, u'pronoooappos': 108, u'pronooodobj': 76, u'partooopunct': 172, u'adpoooacomp': 243, u'adpoooamod': 125, u'detooonsubj': 63, u'pronooooprd': 210, u'verboooadvmod': 145, u'verbooonsubj': 59, u'adpooodobj': 204, u'nounoooappos': 49, u'nounooooprd': 81, u'verbooopcomp': 83, u'partoooccomp': 151, u'advoooprt': 175, u'pronoooconj': 158, u'verbooodep': 154, u'propnooocsubjpass': 215, u'adjoooacl': 209, u'adpoooroot': 133, u'propnoooappos': 16, u'nounooocompound': 14, u'adpooodet': 208, u'propnooodobj': 33, u'advooonsubjpass': 128, u'propnoooadvcl': 147, u'partoooaux': 46, u'cconjooocc': 18, u'nounoooadvcl': 105, u'propnooointj': 122, u'propnoooxcomp': 179, u'advooodep': 194, u'propnooocsubj': 193, u'adjoooattr': 79, u'adpooopcomp': 112, u'numooonummod': 13, u'propnooonsubj': 6, u'numoooquantmod': 197, u'verboooparataxis': 161, u'verboooattr': 166, u'cconjooocompound': 221, u'advoooagent': 242, u'verbooonsubjpass': 146, u'pronoooposs': 121, u'numoooroot': 53, u'propnoooroot': 48, u'numoooappos': 68, u'verbooodobj': 116, u'nounooodobj': 15, u'adpooonmod': 87, u'verboooadvcl': 45, u'nounooorelcl': 115, u'verboooamod': 47, u'adpooocompound': 70, u'verboooacomp': 120, u'propnoooattr': 17, u'adpoooconj': 101, u'advoooposs': 168, u'detoooamod': 235, u'verboooacl': 34, u'verbooocase': 144, u'adjooonpadvmod': 157, u'adjooorelcl': 202, u'advoooccomp': 182, u'adjooonummod': 106, u'verbooonmod': 176, u'pronooodative': 181, u'numooopunct': 86, u'numoooposs': 110, u'nounooonmod': 69, u'detoooadvmod': 137, u'detooonsubjpass': 129, u'numooopobj': 25, u'advoooamod': 134, u'nounoooconj': 27, u'detooonmod': 240, u'adjooodobj': 85, u'detooocompound': 97, u'nounooopobj': 7, u'detooopreconj': 140, u'nounoooamod': 55, u'adjoooprep': 177, u'numoooconj': 102, u'detooopunct': 213, u'partoooprep': 152, u'advooodobj': 169, u'propnoooamod': 39, u'pronoooattr': 211, u'adpoooprt': 127, u'cconjoooadvmod': 188, u'detooooprd': 236, u'numooodep': 178, u'pronooonsubjpass': 61, u'adpooocc': 237, u'nounoooroot': 50, u'partoooquantmod': 186, u'nounooonsubjpass': 24, u'nounooopunct': 171, u'advoooquantmod': 142, u'propnoooconj': 23, u'intjoooconj': 233, u'verboooconj': 29, u'propnooonmod': 35, u'advoooacomp': 143, u'numoooccomp': 228, u'nounoooccomp': 103, u'detoooappos': 180, u'advooocc': 241, u'numooonpadvmod': 65, u'adjoooadvcl': 156, u'advooopobj': 89, u'detoooneg': 219, u'advoooconj': 130, u'cconjooodet': 174, u'propnooopobj': 5, u'adpooonsubj': 205, u'propnooocompound': 1, u'numoooacomp': 132, u'adjooodet': 184, u'adjoooconj': 64, u'adjooonmod': 131, u'detooopredet': 229, u'adpooopobj': 109, u'verbooopunct': 148, u'pronooopobj': 82, u'intjooointj': 165, u'nounoooacl': 164, u'numoooattr': 32, u'numoooadvmod': 227, u'propnoooposs': 22, u'nounoooparataxis': 136, u'symoooquantmod': 207, u'advoooappos': 239, u'adjooopredet': 149, u'verboooccomp': 40, u'nounoooposs': 71, u'adjooopunct': 192, u'nounooocase': 201, u'advoooadvmod': 19, u'advoooprep': 198, u'adjoooamod': 11, u'detoooroot': 238}\n"
     ]
    }
   ],
   "source": [
    "print(sim_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = encode_sequences(compl_tokenizer, compl_length, [' '.join(x[\"compl\"]) for i, x in train.iteritems()])\n",
    "trainY = encode_sequences(sim_tokenizer, sim_length, [' '.join(x[\"sim\"]) for i, x in train.iteritems()])\n",
    "trainY = encode_output(trainY, sim_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = encode_sequences(compl_tokenizer, compl_length, [' '.join(x[\"compl\"]) for i, x in test.iteritems()])\n",
    "testY = encode_sequences(sim_tokenizer, sim_length, [' '.join(x[\"sim\"]) for i, x in test.iteritems()])\n",
    "testY_rem = testY\n",
    "testY = encode_output(testY, sim_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5728 89 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print len(trainX), len(trainY[0]), trainY[555]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(compl_vocab_size, sim_vocab_size, compl_length, sim_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17867 samples, validate on 4441 samples\n",
      "Epoch 1/20\n",
      "17867/17867 [==============================] - 317s 18ms/step - loss: 1.1354 - val_loss: 0.8442\n",
      "Epoch 2/20\n",
      "17867/17867 [==============================] - 318s 18ms/step - loss: 0.8319 - val_loss: 0.8287\n",
      "Epoch 3/20\n",
      "17867/17867 [==============================] - 320s 18ms/step - loss: 0.8379 - val_loss: 0.8339\n",
      "Epoch 4/20\n",
      "17867/17867 [==============================] - 322s 18ms/step - loss: 0.8135 - val_loss: 0.8076\n",
      "Epoch 5/20\n",
      "17867/17867 [==============================] - 313s 18ms/step - loss: 0.7977 - val_loss: 0.7977\n",
      "Epoch 6/20\n",
      "17867/17867 [==============================] - 318s 18ms/step - loss: 0.7896 - val_loss: 0.8021\n",
      "Epoch 7/20\n",
      "17867/17867 [==============================] - 315s 18ms/step - loss: 0.7863 - val_loss: 0.8046\n",
      "Epoch 8/20\n",
      "17867/17867 [==============================] - 316s 18ms/step - loss: 0.7817 - val_loss: 0.7872\n",
      "Epoch 9/20\n",
      "17867/17867 [==============================] - 318s 18ms/step - loss: 0.7772 - val_loss: 0.8162\n",
      "Epoch 10/20\n",
      "17867/17867 [==============================] - 318s 18ms/step - loss: 0.7709 - val_loss: 0.7765\n",
      "Epoch 11/20\n",
      "17867/17867 [==============================] - 321s 18ms/step - loss: 0.7625 - val_loss: 0.7680\n",
      "Epoch 12/20\n",
      "17867/17867 [==============================] - 318s 18ms/step - loss: 0.7564 - val_loss: 0.7672\n",
      "Epoch 13/20\n",
      "17867/17867 [==============================] - 313s 18ms/step - loss: 0.7509 - val_loss: 0.7595\n",
      "Epoch 14/20\n",
      "17867/17867 [==============================] - 958s 54ms/step - loss: 0.7480 - val_loss: 0.7577\n",
      "Epoch 15/20\n",
      "17867/17867 [==============================] - 308s 17ms/step - loss: 0.7426 - val_loss: 0.7543\n",
      "Epoch 16/20\n",
      "17867/17867 [==============================] - 289s 16ms/step - loss: 0.7392 - val_loss: 0.7506\n",
      "Epoch 17/20\n",
      "17867/17867 [==============================] - 282s 16ms/step - loss: 0.7349 - val_loss: 0.7495\n",
      "Epoch 18/20\n",
      "17867/17867 [==============================] - 298s 17ms/step - loss: 0.7323 - val_loss: 0.7452\n",
      "Epoch 19/20\n",
      "17867/17867 [==============================] - 279s 16ms/step - loss: 0.7283 - val_loss: 0.7404\n",
      "Epoch 20/20\n",
      "17867/17867 [==============================] - 279s 16ms/step - loss: 0.7249 - val_loss: 0.7393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf63fad790>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=20, batch_size=98, validation_data=(testX, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5652 100\n"
     ]
    }
   ],
   "source": [
    "print len(trainX), len(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4441\n"
     ]
    }
   ],
   "source": [
    "print len(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.20282180e-04 3.84107083e-01 6.12753816e-03 ... 7.44440456e-07\n",
      "  7.95420419e-06 9.38735752e-07]\n",
      " [5.77302271e-05 6.03535295e-01 1.54411513e-02 ... 2.66166637e-07\n",
      "  4.01458919e-06 2.58714238e-07]\n",
      " [4.74725275e-05 5.34867585e-01 7.53834397e-02 ... 8.87111256e-08\n",
      "  2.98961118e-06 7.85386334e-08]\n",
      " ...\n",
      " [9.98717904e-01 2.24181087e-04 1.09373803e-04 ... 3.07271897e-08\n",
      "  8.07925904e-09 1.03261018e-08]\n",
      " [9.98793840e-01 2.08598518e-04 1.04964172e-04 ... 2.88884845e-08\n",
      "  7.64654384e-09 9.62586277e-09]\n",
      " [9.98854518e-01 1.96412133e-04 1.01571386e-04 ... 2.73891700e-08\n",
      "  7.29256877e-09 9.06798014e-09]]\n"
     ]
    }
   ],
   "source": [
    "print translation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(integers, tokenizer):\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detooodet nounooonsubj adpoooprep propnooopobj propnoooappos verboooroot propnooocompound propnoooattr adjooonsubjpass verboooauxpass verbooorelcl adpoooagent detooodet propnooocompound propnooocompound propnooopobj verboooacl adpoooprep detooodet propnooocompound propnooocompound propnooopobj advoooadvmod propnooonsubjpass verboooauxpass verbooorelcl\n",
      "\n",
      "propnooocompound propnooonsubj verboooroot detooodet nounoooattr adpoooprep propnooopobj propnoooappos propnooonsubjpass adjooonsubj verbooorelcl nounoooattr adpoooprep propnooopobj verboooauxpass verboooroot adpoooagent detooodet propnooocompound propnooocompound propnooopobj detooodet propnooocompound propnooocompound propnooonsubj verboooroot adpoooprep detooodet propnooocompound propnooocompound propnooopobj propnooonsubjpass verboooauxpass verboooroot adpoooprep propnooopobj\n",
      "\n",
      "detooodet propnooocompound verboooroot adpoooprep verboooroot propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound propnooocompound\n"
     ]
    }
   ],
   "source": [
    "ff = 999\n",
    "# print len()\n",
    "print predict_sequence(testX[ff], compl_tokenizer)\n",
    "print ''\n",
    "print predict_sequence(testY_rem[ff], sim_tokenizer)\n",
    "print ''\n",
    "# print predict_sequence(testY[ff], sim_tokenizer)\n",
    "integers = [argmax(vector) for vector in translation[ff]]\n",
    "generated = predict_sequence(integers, sim_tokenizer)\n",
    "print generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
