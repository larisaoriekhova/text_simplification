{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import bz2\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "compl_to_sim = {}\n",
    "source_file = bz2.BZ2File(\"tokenized_geougraphy.txt.bz2\", \"r\")\n",
    "# with open(\"tokenized_geougraphy.txt\", \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "for line in source_file:\n",
    "    texts = line.split('\\t')\n",
    "\n",
    "    if len(texts) != 2:\n",
    "        continue\n",
    "    try:\n",
    "        sim_u = texts[0].encode('utf-8')\n",
    "        compl_u = texts[1].encode('utf-8')\n",
    "    except:\n",
    "        continue\n",
    "    compl_to_sim[texts[0]] = texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8381"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(compl_to_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some words does not make sense (or change sense) when used without dependent clause.\n",
    "# For example, \"part\" in \"part of the U.S.A.\" or \"length\" in \"length of 4000\".\n",
    "# This function builds a clause required to save such information.\n",
    "\n",
    "def getReasonableChunk(word, doc):\n",
    "    chunk_words = []\n",
    "    if word.pos_ == 'PROPN':\n",
    "        try:\n",
    "            chunks = list(x for x in doc.noun_chunks if x.root == word)[0]\n",
    "        except:\n",
    "            chunks = [word]\n",
    "        chunk_words.extend(list(x for x in chunks))\n",
    "        inner_conj = [x for x in word.children if x.dep_ == 'cc']\n",
    "        if len(inner_conj) > 0:\n",
    "            chunk_words.extend(inner_conj)\n",
    "            for conj_propn in list(x for x in word.children if (x.dep_ == 'conj') \\\n",
    "                                   and (x.pos_ == 'PROPN')):\n",
    "                chunk_words.extend(getReasonableChunk(conj_propn, doc))\n",
    "\n",
    "    elif word.pos_ == 'NOUN':\n",
    "        nummods = list(x for x in word.children if x.dep_ == 'nummod')\n",
    "        chunk_words.append(word)\n",
    "        chunk_words.extend(nummods)\n",
    "        \n",
    "        dets = list(x for x in word.children if x.dep_ == 'det')\n",
    "        chunk_words.extend(dets)\n",
    "        \n",
    "        compunds = list(x for x in word.children if x.dep_ == 'compound')\n",
    "        for c in compunds:\n",
    "            chunk_words.extend(getReasonableChunk(c, doc))\n",
    "            \n",
    "        optional_mods = list(x for x in word.children if (x.dep_ == 'amod') \\\n",
    "                             and (x.pos_ in ['ADJ', 'VERB']))\n",
    "        r = random.random()\n",
    "        if r > 0.5:\n",
    "            for x in optional_mods:\n",
    "                chunk_words.extend([t for t in x.subtree])\n",
    "        \n",
    "    elif word.text.lower() in ['which', 'that']:\n",
    "        replacer = replaceWhich(word, doc)\n",
    "        if replacer is not None:\n",
    "            chunk_words.extend(getReasonableChunk(replacer, doc))\n",
    "        else:\n",
    "            chunk_words.append(word)\n",
    "    elif word.text.lower() == 'who':\n",
    "        replacer = replaceWhich(word, doc)\n",
    "        if replacer is not None:\n",
    "            chunk_words.extend(getReasonableChunk(replacer, doc))     \n",
    "        else:\n",
    "            chunk_words.append(word)\n",
    "    else:\n",
    "        chunk_words.append(word)\n",
    "\n",
    "        \n",
    "    prepositions = list(x for x in word.children if x.dep_ == 'prep')\n",
    "    pobjects = []\n",
    "    for prep in prepositions:\n",
    "        pobjects = list(x for x in prep.children if x.dep_ == 'pobj')\n",
    "        if len(pobjects) > 0:\n",
    "            chunk_words.append(prep)\n",
    "            for s in pobjects:\n",
    "                chunk_words.extend(getReasonableChunk(s, doc))\n",
    "                    \n",
    "    return chunk_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of lists of words to text (form readable sentences)\n",
    "def wordsToSimpleText(sentence_words):\n",
    "    text = ''\n",
    "    for words in sentence_words:\n",
    "        sentence_text = ''\n",
    "        for word in words:\n",
    "            sentence_text += word.text + ' '\n",
    "            \n",
    "        if len(sentence_text) > 0:\n",
    "            sentence_text = sentence_text[0].upper() + sentence_text[1:]\n",
    "            sentence_text += '. '\n",
    "            \n",
    "            text += sentence_text\n",
    "            \n",
    "    text = text.replace(\" which \", \" this \")\n",
    "    text = text.replace(\"Which \", \"This \")\n",
    "    text = text.replace(\" where\", \" here\")\n",
    "    text = text.replace(\"Where \", \"Here \")\n",
    "    text = text.replace(\"What 's\", \"It 's\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 1\n",
    "# build set of simple sentences based on each verb's subtree in a complex sentence\n",
    "# input: spacy.doc with complex sentences\n",
    "# output: text with one or more simple sentences\n",
    "\n",
    "def subjVerbObjToSentence(sentence, verb_simple_texts):\n",
    "    verbs = list(x for x in sentence if x.pos_ == 'VERB')\n",
    "    new_sentences = []\n",
    "    for v in verbs:\n",
    "        new_sent_words = [v]\n",
    "        has_sence = False\n",
    "        \n",
    "        ouxilaries = list(x for x in v.children if (x.dep_ == 'aux') or (x.dep_ == 'auxpass'))\n",
    "        new_sent_words.extend(ouxilaries)\n",
    "        \n",
    "        advmodes = list(x for x in v.children if (x.dep_ == 'advmod') and (x.text in ['where']))\n",
    "        new_sent_words.extend(advmodes)\n",
    "\n",
    "        subjects = list(x for x in v.children if (x.dep_ == 'nsubj') or (x.dep_ == 'nsubjpass'))\n",
    "        if len(subjects) > 0:\n",
    "            has_sence = True\n",
    "            for s in subjects:\n",
    "                new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "\n",
    "        attributes = list(x for x in v.children if (x.dep_ == 'attr'))\n",
    "        if len(attributes) > 0:\n",
    "            has_sence = True\n",
    "            for s in attributes:\n",
    "                new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "\n",
    "        negs = list(x for x in v.children if (x.dep_ == 'neg'))\n",
    "        if len(negs) > 0:\n",
    "            for s in negs:\n",
    "                new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "        \n",
    "        dobjects = list(x for x in v.children if x.dep_ in ['dobj', 'oprd', 'acomp'])\n",
    "        if len(dobjects) > 0:\n",
    "            has_sence = True\n",
    "            for s in dobjects:\n",
    "                new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "   \n",
    "        prepositions = list(x for x in v.children if x.dep_ == 'prep')\n",
    "        pobjects = []\n",
    "        for prep in prepositions:\n",
    "            pobjects = list(x for x in prep.children if x.dep_ == 'pobj')\n",
    "            if len(pobjects) > 0:\n",
    "                has_sence = True\n",
    "                new_sent_words.append(prep)\n",
    "                for s in pobjects:\n",
    "                    new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "                    \n",
    "        agents = list(x for x in v.children if x.dep_ == 'agent')\n",
    "        pobjects = []\n",
    "        for prep in agents:\n",
    "            pobjects = list(x for x in prep.children if x.dep_ == 'pobj')\n",
    "            if len(pobjects) > 0:\n",
    "                has_sence = True\n",
    "                new_sent_words.append(prep)\n",
    "                for s in pobjects:\n",
    "                    new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "        if (v.dep_ == 'conj') and (len([x for x in v.children \\\n",
    "                                        if x.dep_ in ['nsubj', 'nsubjpass']]) == 0):\n",
    "            subjects = list(x for x in v.head.children \\\n",
    "                            if (x.dep_ in ['nsubj', 'nsubjpass', 'aux']))\n",
    "            if len(subjects) > 0:\n",
    "                has_sence = True\n",
    "                for s in subjects:\n",
    "                    new_sent_words.extend(getReasonableChunk(s, sentence))\n",
    "        \n",
    "        if v.dep_ in ['xcomp', 'advcl']:\n",
    "            h = v\n",
    "            i = 0\n",
    "            while (True):\n",
    "                i += 1\n",
    "                if i > 5 : \n",
    "                    break\n",
    "                h = h.head\n",
    "                new_sent_words.append(h)\n",
    "                \n",
    "                if h.pos_ == 'VERB':\n",
    "                    has_sence = True\n",
    "                    break\n",
    "            dobjs = [x for x in h.children if x.dep_ == 'dobj']\n",
    "            new_sent_words.extend(dobjs)\n",
    "\n",
    "        new_sent_words = sorted(new_sent_words, key=lambda x: x.i)\n",
    "        \n",
    "        if has_sence == True:\n",
    "            new_sentences.append(new_sent_words)\n",
    "        if has_sence == True:\n",
    "            simple_text = wordsToSimpleText([new_sent_words])\n",
    "            if v.dep_ == 'acl':\n",
    "                simple_text = \"It is \" + simple_text\n",
    "            verb_simple_texts.append(simple_text)\n",
    "    return verb_simple_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 2\n",
    "# Nouns with some related words sometimes bring some imporant \n",
    "# information skipped in previous rule to keep sentence simple.\n",
    "# This function forms separate sentences for such nouns.\n",
    "def adjNounToSentence(sentence, noun_simple_texts):\n",
    "    nouns = list(x for x in sentence if x.pos_ == 'NOUN')\n",
    "    for n in nouns:\n",
    "        new_sent_words = [n]\n",
    "        has_sence = False\n",
    "        \n",
    "        exceptions = ['many', 'more', 'other']\n",
    "        optional_mods = list(x for x in n.children if (x.dep_ == 'amod') \\\n",
    "                             and (x.pos_ in ['ADJ', 'VERB']) and (x.text.lower() not in exceptions))\n",
    "\n",
    "        verb = \"is\"\n",
    "        if n.tag_ == 'NNS':\n",
    "            verb = \"are\"\n",
    "                \n",
    "        add_mods = []\n",
    "        for mod in optional_mods:\n",
    "            add_mods.extend(list(x for x in mod.subtree))\n",
    "\n",
    "        if len(add_mods) > 0:\n",
    "            new_sent_words = sorted(add_mods, key=lambda x: x.i)\n",
    "            noun_chunk = [n]\n",
    "            noun_chunk = sorted(noun_chunk, key=lambda x: x.i)\n",
    "\n",
    "            new_sent_texts = [x.text for x in noun_chunk]\n",
    "            new_sent_texts.append(verb)\n",
    "            new_sent_texts.extend([x.text for x in new_sent_words])\n",
    "            \n",
    "            r = random.random()\n",
    "            if r > 0.5:\n",
    "                \n",
    "                sent_text = ' '.join(new_sent_texts)\n",
    "                sent_text = sent_text[0].upper() + sent_text[1:]\n",
    "                sent_text += ' . '\n",
    "\n",
    "                noun_simple_texts[n.text] = sent_text\n",
    "\n",
    "        appos = list(x for x in n.children if (x.dep_ == 'appos'))\n",
    "        for ap in appos:\n",
    "            new_sent_words = sorted([x for x in ap.subtree], key=lambda x: x.i)\n",
    "            sent_text = n.text + ' ' + verb + ' ' \\\n",
    "            + ' '.join([x.text for x in new_sent_words]) + '. '\n",
    "            sent_text = sent_text[0].upper() + sent_text[1:]\n",
    "            noun_simple_texts[n.text] = sent_text\n",
    "            \n",
    "        in_prep = list(x for x in n.children if (x.dep_ == 'prep') and \\\n",
    "                       (x.text in ['in', 'at', 'on']))\n",
    "        for ap in in_prep:\n",
    "            pobjs = list(x for x in ap.children if x.dep_ == 'pobj')\n",
    "            for p in pobjs:\n",
    "                new_sent_words = sorted([x for x in p.subtree], key=lambda x: x.i)\n",
    "                new_sent_words = getReasonableChunk(p, sentence)\n",
    "                sent_text = n.text + ' ' + verb + ' ' + ap.text + ' ' \\\n",
    "                + ' '.join([x.text for x in new_sent_words]) + ' . '\n",
    "                sent_text = sent_text[0].upper() + sent_text[1:]\n",
    "                noun_simple_texts[n.text] = sent_text\n",
    "            \n",
    "    return noun_simple_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 3\n",
    "# Geography texts often contain constructions like \"City, Region, Country\" \n",
    "# that describe that first entity belongs (is located inside) to second, \n",
    "# and second one belongs to third.\n",
    "# This function forms full sentences to describe such facts.\n",
    "# Also, similar to rule2 is used for proper nouns.\n",
    "\n",
    "def propnConjToSentence(sentence, noun_simple_texts):\n",
    "    nouns = list(x for x in sentence if x.pos_ == 'PROPN')\n",
    "    parts = [' is in ', ' is located in ']\n",
    "    for n in nouns:\n",
    "        appos = list(x for x in n.children if (x.dep_ == 'appos'))\n",
    "        for ap in appos:\n",
    "            new_sent_words = sorted([x for x in ap.subtree], key=lambda x: x.i)\n",
    "            sent_text = n.text + ' is ' + ' '.join([x.text for x in new_sent_words]) + '. '\n",
    "            sent_text = sent_text[0].upper() + sent_text[1:]\n",
    "            noun_simple_texts[n.text] = sent_text\n",
    "            \n",
    "        if len([x for x in n.children if x.dep_ == 'cc']) > 0:\n",
    "            continue\n",
    "        for aggr in list(x for x in n.children if (x.dep_ in ['conj', 'appos']) \\\n",
    "                         and (x.pos_ == 'PROPN')):\n",
    "        \n",
    "            sent_text = n.text + random.choice(parts) + aggr.text + ' . '\n",
    "\n",
    "            noun_simple_texts[n.text] = sent_text\n",
    "\n",
    "    return noun_simple_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 4\n",
    "# Coreference resolution for which, that\n",
    "def replaceWhich(token, doc):\n",
    "    steps = 0\n",
    "    while token.dep_ != 'root':\n",
    "        steps += 1\n",
    "        if steps > 50: \n",
    "            break\n",
    "        token = token.head\n",
    "        if token.pos_ in ['NOUN', 'PROPN']:\n",
    "            return token\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule 5\n",
    "# Coreference resolution for Who\n",
    "def replaceWho(token, doc):\n",
    "    steps = 0\n",
    "    while token.dep_ != 'root':\n",
    "        steps += 1\n",
    "        if steps > 50: \n",
    "            break\n",
    "        token = token.head\n",
    "        if token.pos_ in ['PROPN']:\n",
    "            return token\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex sentence:\n",
      "NewJersey was the site of more RevolutionaryWar battles than any other state .\n",
      "\n",
      " simple text given:\n",
      "In 1776 , GeorgeWashington crossed the DelawareRiver . He and his men landed in Trenton , NewJersey .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read and display random data item\n",
    "compl, sim = random.choice(list(compl_to_sim.items()))\n",
    "\n",
    "print 'complex sentence:'\n",
    "print compl\n",
    "print '\\n simple text given:'\n",
    "print sim\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex sentence:\n",
      "NewJersey was the site of more RevolutionaryWar battles than any other state .\n",
      "\n",
      "simple text given:\n",
      "In 1776 , GeorgeWashington crossed the DelawareRiver . He and his men landed in Trenton , NewJersey .\n",
      "\n",
      "\n",
      "simple text generated: \n",
      "NewJersey was the site of more battles than any other state . \n"
     ]
    }
   ],
   "source": [
    "doc = nlp(compl.decode('utf-8'), disable=['ner', 'textcat'])\n",
    "verb_s_texts = []\n",
    "verb_s_texts = subjVerbObjToSentence(doc, verb_s_texts)\n",
    "noun_s_texts = {}\n",
    "noun_s_texts = adjNounToSentence(doc, noun_s_texts)\n",
    "noun_s_texts = propnConjToSentence(doc, noun_s_texts)\n",
    "\n",
    "simple_text = ''\n",
    "last_found = 0\n",
    "for sent in verb_s_texts:\n",
    "    simple_text += sent\n",
    "    st = sent.lower()\n",
    "    for n in noun_s_texts.keys():\n",
    "        check_sent = noun_s_texts[n].lower().replace(\" is \", \" \")\n",
    "        check_sent = check_sent.replace(\".\", \"\")\n",
    "        if (st.find(n.lower()) >= 0) and (st.find(check_sent) < 0):\n",
    "            simple_text += noun_s_texts[n]\n",
    "            noun_s_texts[n] = ''\n",
    "            break\n",
    "\n",
    "print 'complex sentence:'\n",
    "print compl\n",
    "print '\\nsimple text given:'\n",
    "print sim\n",
    "print '\\nsimple text generated: '\n",
    "print simple_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
